\documentclass[10pt, oneside]{article} 
\usepackage{amsmath, amsthm, amssymb, calrsfs, wasysym, verbatim, bbm, color, graphics, geometry}
\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage[cache=false]{minted}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{url}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{booktabs}

\usetikzlibrary{matrix,arrows,automata}

\geometry{tmargin=.75in, bmargin=.75in, lmargin=.75in, rmargin = .75in}  

\theoremstyle{remark}
\newtheorem*{example}{Przykład}


% Cormen's cost analysis
\newcommand{\TITLE}[1]{\item[#1]}
\renewcommand{\algorithmiccomment}[1]{$/\!/$ \parbox[t]{4.5cm}{\raggedright #1}}
% ugly hack for for/while
\newbox\fixbox
\renewcommand{\algorithmicdo}{\setbox\fixbox\hbox{\ {} }\hskip-\wd\fixbox}
% end of hack
\newcommand{\algcost}[2]{\strut\hfill\makebox[1.5cm][l]{#1}\makebox[4cm][l]{#2}}



\title{Sieci neuronowe -- notatki do kursu}
\author{mgr. inż Dominik Filipiak}
\date{Rok akademicki 2019/2020}

\begin{document}

\maketitle
\tableofcontents

\vspace{.25in}

Dokument ten jest pomocą dla prowadzącego i nie zastępuje w żaden sposób podręcznika akademickiego.
W dużej części bazuje on na: M. Nielsen, \emph{Neural Networks and Deep Learning}\footnote{\url{http://neuralnetworksanddeeplearning.com}}, Determination Press, 2015.

\section{Wprowadzenie do sieci neuronowych}
\subsection{Wielowarstwowy perceptron}

Zdefiniujmy \textbf{perceptron}, matematyczną reprezentację neuronu, który dla trzech binarnych wagi $x_1, x_2, x_3$, gdzie każda z nich to 0 lub 1, zwraca binarne wyjście (czyli również 0 lub 1).
\begin{figure}[htpb]
	\centering
	\includegraphics[width=.3\linewidth]{figures/tikz0}
	\caption{Perceptron z trzema binarnymi wejściami. Źródło: Nielsen (2015)}
\end{figure}
Możemy wprowadzić wagi $w_i$, które określą znaczenie każdego z wejść.
Jego wyjście będzie teraz równe ważonej sumie $\sum_j w_j x_j$.
W ogólności możemy to zapisać jako:
\begin{eqnarray}
  \mbox{output} & = & \left\{ \begin{array}{ll}
      0 & \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\
      1 & \mbox{if } \sum_j w_j x_j > \mbox{ threshold}
      \end{array} \right.
\end{eqnarray}

\begin{example}
Rozważmy problem decyzyjny -- czy wybrać się na festiwal?
Załóżmy, że zadecydują o tym trzy czynniki:
\begin{itemize}
	\item Czy pogoda jest dobra?
	\item Czy mój chłopak/dziewczyna pójdzie ze mną?
	\item Czy da się tam dojechać komunikacją miejską?
\end{itemize}
Oznaczymy odpowiedzi na te pytania. Niech $x_1 = 1$ jeżeli pogoda jest dobra, $x_1 = 0$ w przeciwnym razie.
Podobnie postąpimy z kolejnymi pytaniami.
Jeżeli bardzo chcemy iść na festiwal (więc możemy iść sami i zrezygnować z wygód komunikacji), ale nie znosimy złej pogody, to możemy to oznaczyć jako $w_1=6$, $w_2=2$, $w_3=2$.
Jeżeli ustawimy próg w sieci równy 5, to faktycznie odzwierciedli to nasze przekonania -- partner/partnerka oraz komunikacja miejska nie mają wpływu na wynik, ma go tylko pogoda.
Gdybyśmy natomiast zmniejszyli próg do 3, byłby to model, który reprezentowałby większą skłonność do wyjścia na festiwal.
\end{example}

Uprośćmy teraz nieco notację, a dokładniej warunek $\sum_j w_j x_j > \mbox{threshold}$.
Najpierw zapiszmy $\sum_j w_j x_j$ jako iloczyn skalarny $w \cdot x \equiv \sum_j w_j x_j$.
Próg (threshold) zastąpimy przez $b$ (ang. \emph{bias}), co fachowo nazywamy parametrem obciążenia transformacji afinicznej, gdzie $b \equiv -\mbox{threshold}$.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=.5\linewidth]{figures/tikz1}
	\caption{Prosta sieć. Źródło: Nielsen (2015)}
\end{figure}

\begin{eqnarray}
  \mbox{output} = \left\{ 
    \begin{array}{ll} 
      0 & \mbox{if } w\cdot x + b \leq 0 \\
      1 & \mbox{if } w\cdot x + b > 0
    \end{array}
  \right.
\end{eqnarray}



\subsection{Funkcja sigmoidalana}

W ogólności chcielibyśmy, żeby mała zmiana wag odpowiadała małej zmianie na wyjściu sieci.
W naszym przypadku mała zmiana potrafi jednak wszystko obrócić do góry nogami, tj. zamienić 0 na wyjściu w 1.
Modyfikacja wag jest więc problemem -- możemy sobie z nim poradzić wprowadzając nowy rodzaj neuronu, \textbf{funkcję sigmoidalną}.
\begin{figure}[htpb]
	\centering
	\includegraphics[width=.5\linewidth]{figures/tikz8}
	\caption{Prosta sieć. Źródło: Nielsen (2015)}
\end{figure}
Podobnie jak w perceptronie, taka funkcja ma ileś wejść -- tym razem jednak nie będą one w przedziale $[0, 1]$ (zamiast tylko 0 i 1) -- na przykład 0,638.
Zostawiamy również wagi oraz obciążenie.
Wyjście, podobnie jak wejście, znajduje się w zbiorze $[0, 1]$.
Określamy je jako $\sigma(w \cdot x+b)$, gdzie $\sigma$ definiujemy następująco:
\begin{eqnarray} 
  \sigma(z) \equiv \frac{1}{1+e^{-z}}.
\end{eqnarray}
Podstawiając dosłownie:
\begin{eqnarray} 
  \frac{1}{1+e^{\left(-\sum_j w_j x_j-b\right)}}.
\end{eqnarray}
Zauważmy, że jeżeli $z\equiv w \cdot x + b$ jest pewną dużą liczbą, to $e^{-z} \approx 0$ i tym samym $\sigma(z) \approx 1$.
Jeżeli natomiast $z$ jest \emph{bardzo} ujemną wartością, to $e^{-z} \rightarrow \infty$, dzięki czemu $\sigma(z) \approx 0$ -- patrz rys. \ref{fig:sigmoid} oraz \ref{fig:step}.
\begin{figure}[htpb]
	\centering
	\includegraphics[width=.4\linewidth]{figures/sigmoid}
	\caption{Sigmoidalna funkcja aktywacji. Źródło: Nielsen (2015)}
	\label{fig:sigmoid}
\end{figure}
\begin{figure}[htpb]
	\centering
	\includegraphics[width=.4\linewidth]{figures/step}
	\caption{Skokowa funkcja aktywacji. Źródło: Nielsen (2015)}
	\label{fig:step}
\end{figure}

Zastępując skokową funkcję przez funkcję sigmoidalną, uzyskujemy pewną matematyczną właśność.
Sigmoida jest funkcją gładką, tj. jest ona różniczkowalna nieskończenie wiele razy na swoim przedziale.
Gładkość implikuje też inne właściwości -- małe zmiany $\Delta w_j$ oraz $\Delta b$ powinny skutkować niewielką zmianą na wyjściu, co można aproksymować w następujący sposób:
\begin{eqnarray} 
  \Delta \mbox{output} \approx \sum_j \frac{\partial \, \mbox{output}}{\partial w_j}
  \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b} \Delta b,
\end{eqnarray}

W ogólności funkcję $f$ postaci $f(w \cdot x + b)$ będziemy nazywać \textbf{funkcją aktywacji}.
Sieci wykorzystujące sigmoidalną funkcję aktywacji nazywane są czasami \textbf{wielowarstwowymi perceptronami} (ang. \emph{MLP}).
Funkcja sigmoidalna ma też bardzo ładną pochodną: $\sigma'(z) = \sigma(z)(1-\sigma(z))$.
\begin{align}
\dfrac{d}{dz} \sigma(z) &= \dfrac{d}{dz} \left[ \dfrac{1}{1 + e^{-z}} \right] \\
&= \dfrac{d}{dz} \left( 1 + \mathrm{e}^{-z} \right)^{-1} \\
&= -(1 + e^{-z})^{-2}(-e^{-z}) \\
&= \dfrac{e^{-z}}{\left(1 + e^{-z}\right)^2} \\
&= \dfrac{1}{1 + e^{-z}\ } \cdot \dfrac{e^{-z}}{1 + e^{-z}}  \\
&= \dfrac{1}{1 + e^{-z}\ } \cdot \dfrac{(1 + e^{-z}) - 1}{1 + e^{-z}}  \\
&= \dfrac{1}{1 + e^{-z}\ } \cdot \left( \dfrac{1 + e^{-z}}{1 + e^{-z}} - \dfrac{1}{1 + e^{-z}} \right) \\
&= \dfrac{1}{1 + e^{-z}\ } \cdot \left( 1 - \dfrac{1}{1 + e^{-z}} \right) \\
&= \sigma(z) \cdot (1 - \sigma(z))
\end{align}

\subsection{Architektura sieci neuronowej}
Sieć neuronowa składa się, w ogólności, z trzech rodzajów warstw: \textbf{wejściowej}, \textbf{ukrytych}, oraz \textbf{wyjściowej}.
\begin{figure}[htpb]
	\centering
	\includegraphics[width=.5\linewidth]{figures/tikz11}
	\caption{Architektura sieci. Źródło: Nielsen (2015)}
\end{figure}
Załóżmy, że chcemy klasyfikować obrazy przedstawiające cyfry zapisane odręcznie o rozmiarze $28 \times 28$ pikseli w skali szarości.
Ponieważ do reprezentacji każdego z nich z łącznie 764 pikseli w skali szarości potrzebujemy tylko jednej liczby (oznaczającej intensywność czerni), będziemy potrzebowali tylu neuronów na wejściu.
Zabawa zaczyna się przy dobieraniu liczby warstw ukrytych, a także ilości neuronów w każdej z nich.
W problemie klasyfikacji możemy dobrać liczbę neuronów na wyjściu tak, żeby reprezentowała liczbę klas -- ponieważ jest 10 cyfr, tylko neuronów będzie miała nasza sieć w warstwie wyjściowej.

\begin{figure}[htpb]
	\centering
	\includegraphics[width=.5\linewidth]{figures/digits}
	\caption{Cyfry zapisane odręcznie. Źródło: MNIST, Nielsen (2015)}
\end{figure}
\begin{figure}[htpb]
	\centering
	\includegraphics[width=.5\linewidth]{figures/digits_separate}
	\caption{Cyfry zapisane odręcznie po segmentacji. Źródło: MNIST, Nielsen (2015)}
\end{figure}
\begin{figure}[htpb]
	\centering
	\includegraphics[width=.1\linewidth]{figures/mnist_first_digit}
	\caption{Pierwsza cyfra. Źródło: MNIST, Nielsen (2015)}
\end{figure}
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=.5\linewidth]{figures/tikz12}
	\caption{Architektura sieci. Źródło: Nielsen (2015)}
\end{figure}



\subsection{Błąd średniokwadratowy jako funkcja kosztu}
Aby rozwiązać problem rozpoznawania odręcznie zapisanych cyfr, wprowadźmy najpierw kilka oznaczeń.
Niech $x$ to będzie nasze wejście, tym razem wektor o 784 wymiarach.
Wyjście oznaczymy przez $y=y(x)$, gdzie np. $y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^T$ oznacza cyfrę 6.
Wprowadźmy teraz \textbf{funkcję kosztu}:
\begin{eqnarray}  C(w,b) \equiv
  \frac{1}{2n} \sum_x \| y(x) - a\|^2.
\end{eqnarray}
We wzorze tym $a$ oznacza wektor wyjść z naszej sieci dla $x$, natomiast $n$ oznacza liczbę rozpatrywanych przypadków testowych dla sieci (tj. liczbę cyfr, na których będziemy ją uczyć).
Ta funkcja jest nazywana często \textbf{błędem średniokwadratowym} (ang. \emph{MSE}).
Jej podstawowa własność to nieujemność.
Warto zauważyć, że jeżeli $y(x)$ jest podobne do $a$, to koszt powinien być blisko zera.
Tym samym określiliśmy \textbf{kryterium optymalizacyjne} naszego algorytmu do trenowania sieci neuronowej -- chcemy, aby zminimalizować koszt $C(w, b)$.
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=.7\linewidth]{figures/valley}
	\caption{Wizualizacja wartości funkcji kosztu dla dwóch wymiarów. Źródło: Nielsen (2015)}
\end{figure}


\subsection{Spadek gradientu}

Wyznaczenie \textbf{globalnego minimum} dla funkcji kosztu w sposób analityczny jest możliwe dla funkcji niewielu zmiennych.
Dla funkcji wielu zmiennych jest to zadanie bardzo trudne.
Możemy się jednak posłużyć prostą heurystyką w poszukiwaniu lokalnego minimum, które często okazuje się być całkiem niezłe.
Wiedząc, że
\begin{eqnarray} 
  \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +
  \frac{\partial C}{\partial v_2} \Delta v_2
\end{eqnarray}
oznaczmy $\Delta v \equiv (\Delta v_1, \Delta v_2)^T$.
Będziemy szukać takich $v$, które sprawią, że $\Delta C$ jest ujemne.
Następnie oznaczmy \textbf{gradient} jako $\nabla C$:
\begin{eqnarray} 
  \nabla C \equiv \left( \frac{\partial C}{\partial v_1}, 
  \frac{\partial C}{\partial v_2} \right)^T.
\end{eqnarray}
Tym samym możemy zapisać wszystko jako:
\begin{eqnarray} 
  \Delta C \approx \nabla C \cdot \Delta v.
\end{eqnarray}
Załóżmy teraz, że:
\begin{equation} 
  \Delta v = -\eta \nabla C,
  \label{eq:delta_v}
\end{equation}
gdzie $\eta$ to pewna wartość (zazwyczaj niewielka, np. 0,001) określająca \textbf{szybkość uczenia się}.
Wiemy, że $\Delta C \approx -\eta \nabla C \cdot \nabla C = -\eta \|\nabla C\|^2$. 
Ponadto, $\| \nabla C \|^2 \geq 0$, co gwarantuje, że $\Delta C \leq 0$ -- a taka własność jest bardzo oczekiwana w naszym problemie.
Tym samym jeżeli będziemy korzystać z równania \ref{eq:delta_v} do obliczenia $\Delta v$, to będziemy przesuwać naszą wirtualną piłkę z $v$ do $v'$ w następujący sposób:
\begin{eqnarray}
  v \rightarrow v' = v -\eta \nabla C.
\end{eqnarray}
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=.7\linewidth]{figures/valley_with_ball}
	\caption{Wizualizacja wartości funkcji kosztu dla dwóch wymiarów -- ilustracja spadku gradientu. Źródło: Nielsen (2015)}
\end{figure}


Dla funkcji wielu zmiennych wszystko działa tak samo, tj. dla $\Delta v = (\Delta v_1, \ldots, \Delta v_m)^T$ mamy:
\begin{eqnarray} 
  \Delta C \approx \nabla C \cdot \Delta v,
\end{eqnarray}
a gradient definiujemy w sposób następujący:
\begin{eqnarray}
  \nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \ldots, 
  \frac{\partial C}{\partial v_m}\right)^T.
\end{eqnarray}
Z taką definicją dojdziemy do tych samych wniosków:
\begin{eqnarray}
  \Delta v = -\eta \nabla C,
\end{eqnarray}
co pozwala wyprowadzić regułę \textbf{spadku gradientu}:
\begin{eqnarray}
  v \rightarrow v' = v-\eta \nabla C.
\end{eqnarray}

Nie ma żadnych przeszkód do zastosowania tego algorytmu dla naszej sieci:
\begin{eqnarray}
  w_k & \rightarrow & w_k' = w_k-\eta \frac{\partial C}{\partial w_k} \\
  b_l & \rightarrow & b_l' = b_l-\eta \frac{\partial C}{\partial b_l}.
\end{eqnarray}


\subsection{Stochastyczny spadek gradientu}

Zauważmy, że funkcja kosztu jest postaci $C = \frac{1}{n} \sum_x C_x$, przez co średni koszt jest równy $C_x \equiv \frac{\|y(x)-a\|^2}{2}$ dla każdej porcji danych.
W praktyce liczy się gradient dla każdej porcji $\nabla C_x$, a następnie uśrednia się je $\nabla C = \frac{1}{n}\sum_x \nabla C_x$ -- jednak trwa to zbyt długo.
\textbf{Stochastyczny spadek gradientu} pozwala na uniknięcie tego problemu -- liczymy gradient dla losowo wybranych $m$ porcji danych.
Oznaczamy je jako $X_1, X_2, \ldots, X_m$ i nazywamy \textbf{minipróbką} (ang. \emph{mini-batch}).
Jeżeli $m$ będzie wystarczająco duże, to:
\begin{eqnarray}
  \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C.
\end{eqnarray}
Zamieniając strony:
\begin{eqnarray}
  \nabla C \approx \frac{1}{m} \sum_{j=1}^m \nabla C_{X_{j}}.
\end{eqnarray}
Wstawiając do reguły uczenia naszej sieci neuronowej mamy finalnie:
\begin{eqnarray} 
  w_k & \rightarrow & w_k' = w_k-\frac{\eta}{m}
  \sum_j \frac{\partial C_{X_j}}{\partial w_k} \\
  b_l & \rightarrow & b_l' = b_l-\frac{\eta}{m}
  \sum_j \frac{\partial C_{X_j}}{\partial b_l},
\end{eqnarray}
gdzie sumy po $X_j$ odnoszą się do aktualnej minipróbki.
Kiedy przeiterujemy po wszystkich dostępnych porcjach danych, to powiemy, że skończyliśmy \textbf{epokę} (ang. \emph{epoch}) treningu.

\section{Algorytm propagacji wstecznej}

\subsection{Zapis macierzowy}
Niech $w_{jk}^{l}$ oznacza wagę dla połączenia z $k$-tego neuronu w $(l-1)$-tej warstwie do $j$-tego neuronu w $l$-tej warstwie.
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=.7\linewidth]{figures/tikz16}
	\caption{Oznaczenia wag. Źródło: Nielsen (2015)}
\end{figure}
Podobnie postąpi z obciążeniami $b_j^l$ i aktywacjami $a_j^l$.
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=.3\linewidth]{figures/tikz17}
	\caption{Oznaczenia obciążeń i aktywacji. Źródło: Nielsen (2015)}
\end{figure}
Z tą notacją nasza funkcja aktywacji prezentuje się następująco:
\begin{eqnarray} 
  a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right),
\end{eqnarray}
gdzie suma jest po $k$ neuronach w $(l-1)$-tej warstwie.
Ponieważ zapis jest dość skomplikowany, to warto skorzystać z formy zwektoryzowanej, tj. zapisać macierz $w^l$ dla każdej warstwy, gdzie każda wpis $w^{l}_{jk}$ to wagi dla $l$-tej warstwy $j$-tej kolumnie i $k$-tym wierszu.
Analogicznie postąpimy z $b_j^l$ i aktywacjami $a_j^l$ -- zdefiniujmy dla nich wektory $b^l$ i $a^l$.
Finalnie pokażmy jeszcze mechanizm wektoryzacji funkcji:
\begin{eqnarray}
  f\left(\left[ \begin{array}{c} 2 \\ 3 \end{array} \right] \right)
  = \left[ \begin{array}{c} f(2) \\ f(3) \end{array} \right]
  = \left[ \begin{array}{c} 4 \\ 9 \end{array} \right],
\end{eqnarray}
dzięki czemu możemy zapisać całe równanie w zgrabnej formie:
\begin{eqnarray} 
  a^{l} = \sigma(w^l a^{l-1}+b^l).
\end{eqnarray}
W ogólności będziemy najpierw liczyć $z^l \equiv w^l a^{l-1}+b^l$, które oznacza ważone wejście do warstwy $l$. Oczywiście $a^l =
\sigma(z^l)$.

\subsection{Założenia dotyczące funkcji kosztu}
Zapiszmy funkcję kosztu używając macierzy:
\begin{eqnarray}
  C = \frac{1}{2n} \sum_x \|y(x)-a^L(x)\|^2,
\end{eqnarray}
gdzie $L$ to liczba warstw (a więc chodzi o ostatnią warstwę).

Aby algorytm propagacji wstecznej zadziałał, musimy poczynić 2 założenia o (dowolnej) funkcji kosztu. Pierwsze z nich mówi, że $C = \frac{1}{n} \sum_x C_x$ (ponieważ algorytm pozwala nam liczyć pochodną cząstkową tylko dla pojedynczych przykładów).
Drugie założenie mówi, że koszt musi dać się zapisać jako funkcja wyjść sieci neuronowej.
W naszym przypadku
\begin{eqnarray}
  C = \frac{1}{2} \|y-a^L\|^2 = \frac{1}{2} \sum_j (y_j-a^L_j)^2,
\end{eqnarray}
co rozwiązuje sprawę.
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=.5\linewidth]{figures/tikz18}
	\caption{Drugie założenie o funkcji kosztu. Źródło: Nielsen (2015)}
\end{figure}

\subsection{Propagacja wsteczna}

Zdefiniujmy \textbf{iloczyn Hadamarda} $s \odot t$ jako iloczyn po elementach obu wektorów, tj. $(s \odot t)_j = s_j t_j$, np.:
\begin{eqnarray}
\left[\begin{array}{c} 1 \\ 2 \end{array}\right] 
  \odot \left[\begin{array}{c} 3 \\ 4\end{array} \right]
= \left[ \begin{array}{c} 1 * 3 \\ 2 * 4 \end{array} \right]
= \left[ \begin{array}{c} 3 \\ 8 \end{array} \right].
\end{eqnarray}

Algorytm propagacji wstecznej pozwoli nam obliczyć $\partial C / \partial w^l_{jk}$ oraz $\partial C / \partial b^l_{j}$ -- potrzebujemy jednak najpierw pośrednich wartości dla wszystkich neuronów.
Oznaczmy $\delta_j^l$ jako błąd w $j$-tym neuronie w $l$-tej warstwie.
Załóżmy, że ten błąd jest reprezentowany przez pewnego demona, który dodaje zawsze $\Delta z^l_j$ do ważonego wejścia danego neuronu, przez co zwraca on $\sigma(z^l_j+\Delta z^l_j)$ zamiast $\sigma(z^l_j)$.
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=.5\linewidth]{figures/tikz19}
	\caption{Demon w $j$-tym neuronie $l$-tej warstwy naszej sieci. Źródło: Nielsen (2015)}
\end{figure}
Demon jest po naszej stronie -- stara się dorzucić do wejścia takie wartości, które zminimalizują koszt. 
Tak więc:
\begin{eqnarray} 
  \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.
\end{eqnarray}
Pokażmy teraz 4 równania związane z propagacją wsteczną.

\subsubsection{Błąd na wyjściu sieci}
Pierwsze równanie prezentuje się następująco:
\begin{eqnarray} 
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j).
\end{eqnarray}
Wyrażenie po lewej mierzy jak szybko zmienia się koszt w ostatniej warstwie, a to po prawej robi to samo z aktywacją.
Dla naszej funkcji $\partial C / \partial a^L_j = (a_j^L-y_j)$.
Możemy wszystko zapisać jako:
\begin{eqnarray} 
  \delta^L = \nabla_a C \odot \sigma'(z^L),
\end{eqnarray}
gdzie $\nabla_a C$ jest wektorem pochodnych cząstkowych ${\partial C}/{\partial a^L_j}$.
Innymi słowy:
\begin{eqnarray} 
  \delta^L = (a^L-y) \odot \sigma'(z^L).
\end{eqnarray}
\subsubsection{Błąd $\delta^l$ w stosunku do błędu $\delta^{l+1}$}
Mamy:
\begin{eqnarray} 
  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l),
\end{eqnarray}
Jeżeli znamy błąd w kolejnej warstwie ($\delta^(l+1)$), to możemy obliczyć $\delta^l$ -- niejako propagując błąd wstecz (od końca).

\subsubsection{Tempo zmian kosztu wobec obciążenia w sieci}
Mamy:
\begin{eqnarray}  
	\frac{\partial C}{\partial b^l_j} = \delta^l_j,
\end{eqnarray}
co można zapisać jako
\begin{eqnarray}
  \frac{\partial C}{\partial b} = \delta.
\end{eqnarray}

\subsubsection{Tempo zmian kosztu wobec wag w sieci}
Mamy:
\begin{eqnarray}
  \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j.
\end{eqnarray}
Równoważnie:
\begin{eqnarray}  
  \frac{\partial C}{\partial w} = a_{\rm in} \delta_{\rm out}.
\end{eqnarray}
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=.2\linewidth]{figures/tikz20}
	\caption{Tempo zmian kosztu wobec wag w sieci. Źródło: Nielsen (2015)}
\end{figure}

\subsection{Algorytm propagacji wstecznej}
Algorytm propagacji wstecznej działa na podstawie poniższych czterech równań:
\begin{align}
	\delta^L &= \nabla_a C \odot \sigma'(z^L)\\
	\delta^l &= ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)\\
	\frac{\partial C}{\partial b^l_j} &= \delta^l_j \\
	\frac{\partial C}{\partial w^l_{jk}} &= a^{l-1}_k \delta^l_j
\end{align}

\begin{algorithm}
    \caption{Algorytm propagacji wstecznej}
    \label{alg:backprop}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
        \Function{Backpropagation}{$x$}
            \State $a^1 \gets x$ \Comment{wejście jest \emph{wyjściem} pierwszej warstwy}
            	\For{$l \gets {2, 3, \dots, L}$}
            		\State $z^{l} \gets w^l a^{l-1}+b^l$
            		\State $a^{l} \gets \sigma(z^{l})$
            	\EndFor
            	\State $\delta^{L} \gets \nabla_a C \odot \sigma'(z^L)$
            	\For{$l \gets {L-1, L-2, \dots, 2}$}
            		\State $\delta^{l} \gets ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^{l})$         
            	\EndFor
            	\State $\frac{\partial C}{\partial w^l_{jk}} \gets a^{l-1}_k \delta^l_j$
            	\State $\frac{\partial C}{\partial b^l_j} \gets \delta^l_j$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

Algorytm ten jest często łączony z innymi, np. SGD.

\begin{algorithm}
    \caption{Algorytm propagacji wstecznej + SGD + minibatch}
    \label{alg:backprop_sgf}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
        \Function{SGD-Backpropagation}{$x$}
        		\For{$x$ in $X_m$} \Comment{minibatch $m$}
	            \State $a^{x,1} \gets x$ \Comment{wejście jest \emph{wyjściem} pierwszej warstwy}
	            	\For{$l \gets {2, 3, \dots, L}$}
	            		\State $z^{x,l} \gets w^l a^{x,l-1}+b^l$
	            		\State $a^{x,l} \gets \sigma(z^{x,l})$
	            	\EndFor
	            	\State $\delta^{x,L} \gets \nabla_a C \odot \sigma'(z^{x,L})$
	            	\For{$l \gets {L-1, L-2, \dots, 2}$}
	            		\State $\delta^{x,l} \gets ((w^{x,l+1})^T \delta^{x,l+1}) \odot \sigma'(z^{x,l})$         
	            	\EndFor
	            	\State $\frac{\partial C}{\partial w^l_{jk}} \gets a^{x,l-1}_k \delta^{x,l_j}$
	            	\State $\frac{\partial C}{\partial b^l_j} \gets \delta^{x,l_j}$
	         \EndFor
	         \For{$l \gets {L, L-2, \dots, 2}$}
	         	\State $w^l \gets w^l-\frac{\eta}{m} \sum_x \delta^{x,l} (a^{x,l-1})^T$
	         	\State $b^l \gets b^l-\frac{\eta}{m}
  \sum_x \delta^{x,l}$
	         \EndFor
        \EndFunction
    \end{algorithmic}
\end{algorithm}


\section{Implementacja prostej sieci neuronowej w NumPy}
\subsection{Linki}
Wejdź na tą\footnote{\url{https://colab.research.google.com/drive/1sVY_zZscj5Xa9XWRrQxf5bBqiv4C4Bwu}} stronę. 
Jest tu zalążek kodu służący do pobierania danych MNIST -- jest tam 60 tys. przykładów.
Pełne rozwiązanie znajdziesz tutaj\footnote{\url{https://colab.research.google.com/drive/1ngZmy01o3GtuwWUj4p5Xrbw7dQd7nd3D}}.
Oryginalny kod jest wzięty z tej\footnote{\url{https://github.com/mnielsen/neural-networks-and-deep-learning.git}} strony.

\subsection{Kod}
Zdefiniujmy najpierw funkcję sigmoidalną:
\begin{minted}{python}
def sigmoid(z):
    return 1.0/(1.0+np.exp(-z))
\end{minted}
oraz jej pochodną
\begin{minted}{python}
    def sigmoid_prime(z):
        """Derivative of the sigmoid function."""
        return sigmoid(z)*(1-sigmoid(z))
\end{minted}
Sieć zacznijmy od konstruktora:
\begin{minted}{python}
import numpy as np

class Network(object):
  def __init__(self, sizes):
    self.num_layers = len(sizes)
    self.sizes = sizes
    self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
    self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]
\end{minted}
więc np. $w=$ \mintinline{python}{net.weights[1]} oznacza wagi pomiędzy drugą i trzecią warstwą.
We wzorach będziemy oznaczać to jako $w_{jk}$, co oznacza wagę dla połączenia między $k$-tym neuronem w trzeciej warstwie a $j$-tym neuronem w drugiej warstwie.
Dzięki takiemu oznaczeniu możemy napisać:
\begin{eqnarray} 
  a' = \sigma(w a + b),
\end{eqnarray}
gdzie $a$ to wektor aktywacji drugiej warstwy, a $a'$ to wektor aktywacji dla trzeciej warstwy.

a następnie zdefiniujmy kolejną metodę:
\begin{minted}{python}
    def feedforward(self, a):
	    for b, w in zip(self.biases, self.weights):
	        a = sigmoid(np.dot(w, a)+b)
	    return a
\end{minted}
Następnie czas na stochastyczny gradient.
\begin{minted}{python}
    def SGD(self, training_data, epochs, mini_batch_size, eta,
            test_data=None):
        if test_data: n_test = len(test_data)
        n = len(training_data)
        for j in xrange(epochs):
            random.shuffle(training_data)
            mini_batches = [
                training_data[k:k+mini_batch_size]
                for k in xrange(0, n, mini_batch_size)]
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta)
            if test_data:
                print "Epoch {0}: {1} / {2}".format(
                    j, self.evaluate(test_data), n_test)
            else:
                print "Epoch {0} complete".format(j)
\end{minted}
Pod zmienną \mintinline{python}{training_data} kryje się lista krotek postaci $(x, y)$, co reprezentuje odpowiednio cyfry do rozpoznania wraz z ich odpowiednimi oznaczeniami.
Zmienna \mintinline{python}{test_data} pozwala na ewaluację sieci po każdej epoce.
W każdej epoce kod partycjonuje dane wejściowe na minipróbki, a każda z nich będzie pojedynczym krokiem dla SGD przetworzonym przez poniższy kod:
\begin{minted}{python}
    def update_mini_batch(self, mini_batch, eta):
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw 
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb 
                       for b, nb in zip(self.biases, nabla_b)]
\end{minted}
Teraz czas na mechanizm propagacji wstecznej:
\begin{minted}{python}
    def backprop(self, x, y):
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        # feedforward
        activation = x
        activations = [x] # list to store all the activations, layer by layer
        zs = [] # list to store all the z vectors, layer by layer
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)
        delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
        for l in xrange(2, self.num_layers):
            z = zs[-l]
            sp = sigmoid_prime(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)
\end{minted}
W tej metodzie ewaluujemy sieć:
\begin{minted}{python}
    def evaluate(self, test_data):
        test_results = [(np.argmax(self.feedforward(x)), y)
                        for (x, y) in test_data]
        return sum(int(x == y) for (x, y) in test_results)
\end{minted}
A ta zwraca $\partial C_x / \partial$
\begin{minted}{python}
    def cost_derivative(self, output_activations, y):
        return (output_activations-y)
\end{minted}


Mamy już wszystko gotowe. 
Trening rozpoczynamy przez wywołanie następującej metody:
\begin{minted}{python}
net = Network([784, 30, 10])
net.SGD(training_data, 30, 10, 3.0, test_data=test_data)	
\end{minted}
Tutaj sieć uczy się za wolno...
\begin{minted}{python}
net = Network([784, 30, 10])
net.SGD(training_data, 30, 10, 0.001, test_data=test_data)	
\end{minted}
...a tutaj $\eta$ jest za duża i niejako omijamy minimum lokalne:
\begin{minted}{python}
net = Network([784, 30, 10])
net.SGD(training_data, 30, 10, 100.0, test_data=test_data)	
\end{minted}

\section{Usprawnienia procesu uczenia się sieci}

\subsection{Powolne uczenie się}

Rozpatrzmy pojedynczy neuron, którego zadaniem jest dla wejścia 1 zwracać 0.
Załóżmy, że $\eta = 0.15$, $w=0.06$, $b=0.09$.
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=.5\linewidth]{figures/learn_mse_0609}
	\caption{Proces uczenia się -- MSE, $\eta = 0.15$, $w=0.06$, $b=0.09$. Źródło: Nielsen (2015)}
\end{figure}
Dla $w=2$, $b=2$ proces uczenia przebiega znacznie wolniej.
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=.5\linewidth]{figures/learn_mse_2020}
	\caption{Proces uczenia się -- MSE, $\eta = 0.15$, $w=2.00$, $b=2.00$. Źródło: Nielsen (2015)}
\end{figure}
Do źródła problemu dojdziemy analizując funkcję kosztu, tj.:
\begin{eqnarray}
  C = \frac{(y-a)^2}{2}
\end{eqnarray}
Pamiętając, że $a = \sigma(z)$ oraz $z = wx +b$ i podstawiając $x=1$ oraz $y=0$ obliczmy pochodne
\begin{eqnarray} 
  \frac{\partial C}{\partial w} & = & (a-y)\sigma'(z) x = a \sigma'(z) \\
  \frac{\partial C}{\partial b} & = & (a-y)\sigma'(z) = a \sigma'(z),
\end{eqnarray}
Zmiana kosztu będzie mała, gdy mała jest $\sigma'(z)$ -- a tak jest blisko 0 i 1.
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=.5\linewidth]{figures/sigmoid2}
	\caption{Sigmoida. Źródło: Nielsen (2015)}
\end{figure}



\subsection{Entropia krzyżowa}
Entropia krzyżowa (ang. \emph{cross-entropy}) to inna (zazwyczaj lepsza od MSE) funkcja kosztu.
Definiujemy ją następująco:
%\begin{figure}[!htpb]
%	\centering
%	\includegraphics[width=.3\linewidth]{figures/tikz29}
%	\caption{Neuron. Źródło: Nielsen (2015)}
%\end{figure}
\begin{eqnarray} 
  C = -\frac{1}{n} \sum_x \left[y \ln a + (1-y ) \ln (1-a) \right],
\end{eqnarray}
Widać, że $C > 0$ -- oba logarytmy są między 0 a 1 -- oraz że wynik zmierza do 0, gdy estymacja jest bliska oryginałowi.
Ogromną zaletą tej funkcji jest uniknięcie problemu spowolnionego uczenia.
Podstawiając $a = \sigma(z)$ i różniczkując po $w$ dostaniemy:
\begin{eqnarray}
  \frac{\partial C}{\partial w_j} & = & -\frac{1}{n} \sum_x \left(
    \frac{y }{\sigma(z)} -\frac{(1-y)}{1-\sigma(z)} \right)
  \frac{\partial \sigma}{\partial w_j} \\
 & = & -\frac{1}{n} \sum_x \left( 
    \frac{y}{\sigma(z)} 
    -\frac{(1-y)}{1-\sigma(z)} \right)\sigma'(z) x_j.
\end{eqnarray}
Po uproszczeniu:
\begin{eqnarray}
  \frac{\partial C}{\partial w_j} & = & \frac{1}{n}
  \sum_x \frac{\sigma'(z) x_j}{\sigma(z) (1-\sigma(z))}
  (\sigma(z)-y).
\end{eqnarray}
Wiedząc, że $\sigma'(z) = \sigma(z)(1-\sigma(z))$
\begin{eqnarray} 
  \frac{\partial C}{\partial w_j} =  \frac{1}{n} \sum_x x_j(\sigma(z)-y).
\end{eqnarray}
To równanie mówi, że nauka będzie przebiegała proporcjonalnie do błędu, tj. im będzie większy, tym większa zmiana w sieci.
Analogicznie postąpimy z obciążeniem:
\begin{eqnarray} 
  \frac{\partial C}{\partial b} = \frac{1}{n} \sum_x (\sigma(z)-y).
\end{eqnarray}

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=.5\linewidth]{figures/learn_ce_0609}
	\caption{Proces uczenia się -- entropia krzyżowa. Źródło: Nielsen (2015)}
\end{figure}
\begin{figure}[!htpb]
	\centering
	\includegraphics[width=.5\linewidth]{figures/learn_ce_2020}
	\caption{Proces uczenia się -- entropia krzyżowa. Źródło: Nielsen (2015)}
\end{figure}
Generalizacja entropii krzyżowej na wiele neuronów wygląda następująco:
\begin{eqnarray}  C = -\frac{1}{n} \sum_x
  \sum_j \left[y_j \ln a^L_j + (1-y_j) \ln (1-a^L_j) \right].
\end{eqnarray}

%Skąd wzięła się entropia krzyżowa?
%\begin{eqnarray} 
%  \frac{\partial C}{\partial w_j} & = & x_j(a-y) \\
%  \frac{\partial C}{\partial b } & = & (a-y).
%\end{eqnarray}
%\begin{eqnarray}
%  \frac{\partial C}{\partial b} = \frac{\partial C}{\partial a} 
%  \sigma'(z).
%\end{eqnarray}
%\begin{eqnarray}
%  \frac{\partial C}{\partial b} = \frac{\partial C}{\partial a} 
%  a(1-a).
%\end{eqnarray}
%\begin{eqnarray}
%  \frac{\partial C}{\partial a} = \frac{a-y}{a(1-a)}.
%\end{eqnarray}
%
%\begin{eqnarray}
%  C = -[y \ln a + (1-y) \ln (1-a)]+ {\rm constant},
%\end{eqnarray}
%
%\begin{eqnarray}
%  C = -\frac{1}{n} \sum_x [y \ln a +(1-y) \ln(1-a)] + {\rm constant},
%\end{eqnarray}
%
%
\subsection{Softmax}


\begin{eqnarray} 
  a^L_j = \frac{e^{z^L_j}}{\sum_k e^{z^L_k}},
\end{eqnarray}
\begin{eqnarray}
  \sum_j a^L_j & = & \frac{\sum_j e^{z^L_j}}{\sum_k e^{z^L_k}} = 1.
\end{eqnarray}

\subsection{Regularyzacja}
\begin{eqnarray} C = -\frac{1}{n} \sum_{xj} \left[ y_j \ln a^L_j+(1-y_j) \ln
(1-a^L_j)\right] + \frac{\lambda}{2n} \sum_w w^2.
\end{eqnarray}

\begin{eqnarray} C = \frac{1}{2n} \sum_x \|y-a^L\|^2 +
  \frac{\lambda}{2n} \sum_w w^2.
\end{eqnarray}

\begin{eqnarray}  C = C_0 + \frac{\lambda}{2n}
\sum_w w^2,
\end{eqnarray}

\begin{eqnarray}  C = C_0 + \frac{\lambda}{n} \sum_w |w|.
\end{eqnarray}

\subsection{Inicjalizacja wag}

\subsection{Optymalizacja hiperparametrów}

Tangens hiperboliczny

\subsection{Inne funkcje aktywacji}
\begin{eqnarray}
  \tanh(z) \equiv \frac{e^z-e^{-z}}{e^z+e^{-z}}.
\end{eqnarray}

\begin{eqnarray} 
  \sigma(z) = \frac{1+\tanh(z/2)}{2},
\end{eqnarray}

ReLU
\begin{eqnarray}
  \max(0, w \cdot x+b).
\end{eqnarray}

\end{document}